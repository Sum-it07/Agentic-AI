{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# TinyLlama Sentiment Fine-Tuning Guide\n",
                "\n",
                "## Overview\n",
                "This notebook walks through fine-tuning the TinyLlama language model on the IMDB movie review dataset so it can perform sentiment analysis. The workflow stays fully local, making it practical for laptops or desktops without high-end GPUs.\n",
                "\n",
                "## Objectives\n",
                "- Prepare the IMDB dataset for supervised fine-tuning\n",
                "- Apply LoRA so only a small fraction of the model parameters are updated\n",
                "- Train and evaluate the adapted model on local hardware\n",
                "- Compare the model before and after fine-tuning to verify the improvement\n",
                "\n",
                "## Notebook Roadmap\n",
                "1. Environment setup: install the libraries and import the components needed later\n",
                "2. Data loading: fetch the IMDB sentiment dataset and format it for instruction-style prompts\n",
                "3. Baseline run: test the original model to capture pre-training behaviour\n",
                "4. Model preparation: configure TinyLlama and attach LoRA adapters\n",
                "5. Training loop: fine-tune on a short subset of labeled reviews\n",
                "6. Inference checks: run the adapted model on held-out examples\n",
                "7. Result review: contrast baseline and fine-tuned predictions\n",
                "\n",
                "## Concepts to Know\n",
                "- LoRA (Low-Rank Adaptation) updates small adapter matrices instead of full model weights\n",
                "- Instruction tuning frames each training example as an instruction-response pair to guide the model\n",
                "- The IMDB dataset contains 50,000 labeled movie reviews split into positive and negative examples\n",
                "- CPU-only training works, but expect longer runtimes than on a GPU\n",
                "\n",
                "## Toolkit\n",
                "- Model: TinyLlama (1.1B parameters) for a manageable yet capable baseline\n",
                "- Training stack: Hugging Face Transformers, TRL (SFTTrainer), and PEFT for adapter management\n",
                "- Hardware: runs on CPU or modest GPU (4 GB+ VRAM recommended)\n",
                "- Outputs: LoRA adapter files in the 10â€“50 MB range instead of multi-gigabyte checkpoints\n",
                "\n",
                "## When This Notebook Helps\n",
                "- You need a reproducible example of lightweight fine-tuning\n",
                "- You plan to adapt language models for sentiment or classification tasks\n",
                "- You want a reference workflow for running production-style experiments on personal hardware\n",
                "- You are documenting a local ML project for classmates or teammates"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup\n",
                "We need `transformers`, `peft` for adapters, `bitsandbytes` for quantization, and `ollama` for baseline comparison."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### What are we installing?\n",
                "- **transformers**: Hugging Face library for loading and managing transformer models\n",
                "- **datasets**: Library for loading and processing datasets like IMDB\n",
                "- **peft**: Parameter-Efficient Fine-Tuning library for LoRA adapters\n",
                "- **bitsandbytes**: Enables 4-bit quantization to reduce memory usage\n",
                "- **ollama**: Local inference engine (for baseline model testing)\n",
                "- **trl**: Transformers Reinforcement Learning library with SFTTrainer\n",
                "- **accelerate**: Multi-GPU and distributed training support\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "[notice] A new release of pip is available: 25.3 -> 26.0\n",
                        "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
                    ]
                }
            ],
            "source": [
                "# Install necessary libraries\n",
                "!pip install -q transformers datasets peft bitsandbytes ollama trl accelerate"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "import ollama\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                "    pipeline\n",
                ")\n",
                "from peft import LoraConfig, PeftModel\n",
                "from trl import SFTTrainer\n",
                "\n",
                "# Suppress excessive warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Core Imports Explained\n",
                "- **os, torch**: System and GPU management\n",
                "- **ollama**: Client for querying the Ollama inference engine\n",
                "- **load_dataset**: Loads datasets from Hugging Face Hub\n",
                "- **AutoModelForCausalLM, AutoTokenizer**: Auto-detect and load the appropriate model/tokenizer\n",
                "- **BitsAndBytesConfig**: Configuration for 4-bit quantization\n",
                "- **TrainingArguments, SFTTrainer**: Core training components\n",
                "- **LoraConfig, PeftModel**: LoRA (Low-Rank Adaptation) configuration and utilities\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load IMDB Dataset\n",
                "We load the IMDB dataset from Hugging Face, which contains movie reviews with sentiment labels."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset Loaded. Size: 100 samples\n",
                        "Sample Entry: {'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', 'label': 0}\n",
                        "\n",
                        "Dataset columns: ['text', 'label']\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    # Load IMDB dataset from Hugging Face\n",
                "    dataset = load_dataset(\"imdb\", split=\"train[:100]\")  # Use only 100 samples for memory efficiency\n",
                "    print(f\"Dataset Loaded. Size: {len(dataset)} samples\")\n",
                "    print(\"Sample Entry:\", dataset[0])\n",
                "    print(\"\\nDataset columns:\", dataset.column_names)\n",
                "except Exception as e:\n",
                "    print(f\"Error loading dataset: {e}\")\n",
                "    print(\"Ensure you have internet connection to download IMDB dataset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading the IMDB Dataset\n",
                "The IMDB dataset contains 50,000 movie reviews, each labeled as positive (1) or negative (0) based on sentiment. We use only 100 samples for faster training on limited hardware.\n",
                "\n",
                "**Dataset structure:**\n",
                "- **text**: The full movie review text\n",
                "- **label**: 0 (negative) or 1 (positive)\n",
                "\n",
                "This cell downloads the dataset and displays sample information to verify loading.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Baseline Inference (Before Training)\n",
                "We use the locally running Ollama instance to check how the base LLaMA 3.2 model performs sentiment analysis *without* fine-tuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Baseline (Ollama) ---\n",
                        "Review Analysis: Analyze the sentiment of the following movie review: 'This film was absolutely amazing! The acting was brilliant, the storyline was engaging, and I couldn't take my eyes off the screen. I highly recommend it to everyone.'\n",
                        "\n",
                        "Response:\n",
                        "The sentiment of this movie review is extremely positive. The reviewer uses superlatives such as \"absolutely amazing\" and \"brilliant\" to describe their experience with the film. They also explicitly state that they \"couldn't take my eyes off the screen,\" which suggests that the movie was so engaging and captivating that it held their full attention.\n",
                        "\n",
                        "The reviewer's use of the phrase \"I highly recommend it to everyone\" further emphasizes their enthusiasm for the film, implying that they think it is a must-see for anyone. There is no criticism or negative comment in the review, only praise and admiration for the movie.\n",
                        "\n",
                        "Overall, the sentiment of this review can be characterized as overwhelmingly positive and enthusiastic.\n"
                    ]
                }
            ],
            "source": [
                "def query_ollama(prompt, model=\"llama3.2\"):\n",
                "    try:\n",
                "        response = ollama.chat(model=model, messages=[\n",
                "            {'role': 'user', 'content': prompt}\n",
                "        ])\n",
                "        return response['message']['content']\n",
                "    except Exception as e:\n",
                "        return f\"Ollama Error: {str(e)}\"\n",
                "\n",
                "# Test Review for Sentiment Analysis\n",
                "test_instruction = \"Analyze the sentiment of the following movie review: 'This film was absolutely amazing! The acting was brilliant, the storyline was engaging, and I couldn't take my eyes off the screen. I highly recommend it to everyone.'\"\n",
                "\n",
                "print(\"--- Baseline (Ollama) ---\")\n",
                "baseline_response = query_ollama(test_instruction)\n",
                "print(f\"Review Analysis: {test_instruction}\\n\")\n",
                "print(f\"Response:\\n{baseline_response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Testing Baseline Model with Ollama\n",
                "Before fine-tuning, we test the untrained LLaMA model to see how it handles sentiment analysis without any training on our specific task.\n",
                "\n",
                "**What this cell does:**\n",
                "1. Defines `query_ollama()` function to communicate with Ollama server\n",
                "2. Creates a test movie review (positive sentiment)\n",
                "3. Gets baseline response from the untrained model\n",
                "4. This serves as a \"before\" comparison point for our fine-tuned model\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Fine-Tuning Setup (QLoRA)\n",
                "\n",
                "We will fine-tune usage Hugging Face Transformers. \n",
                "**Important**: Ollama stores models in GGUF format which isn't directly trainable by standard tools. We will download the base weights for `Llama-3.2-1B-Instruct` (or 3B) from Hugging Face to perform the training, then save the adapter."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "âš  No HF_TOKEN environment variable found\n",
                        "  If using Meta Llama model, you need to:\n",
                        "  1. Accept the license: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\n",
                        "  2. Get your token: https://huggingface.co/settings/tokens\n",
                        "  3. Set HF_TOKEN environment variable or uncomment login() above\n",
                        "  \n",
                        "  Using non-gated TinyLlama model instead (no authentication needed)\n"
                    ]
                }
            ],
            "source": [
                "from huggingface_hub import login\n",
                "\n",
                "# Login to Hugging Face (only needed for gated models like Meta Llama)\n",
                "# Option 1: Interactive login (uncomment and run to use)\n",
                "# login()\n",
                "\n",
                "# Option 2: Use HF_TOKEN environment variable\n",
                "# To use this: Set environment variable HF_TOKEN=your_token_here\n",
                "# Get your token from: https://huggingface.co/settings/tokens\n",
                "import os\n",
                "hf_token = os.getenv('HF_TOKEN')\n",
                "if hf_token:\n",
                "    login(token=hf_token)\n",
                "    print(\"âœ“ Logged in to Hugging Face successfully!\")\n",
                "else:\n",
                "    print(\"âš  No HF_TOKEN environment variable found\")\n",
                "    print(\"  If using Meta Llama model, you need to:\")\n",
                "    print(\"  1. Accept the license: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\")\n",
                "    print(\"  2. Get your token: https://huggingface.co/settings/tokens\")\n",
                "    print(\"  3. Set HF_TOKEN environment variable or uncomment login() above\")\n",
                "    print(\"  \")\n",
                "    print(\"  Using non-gated TinyLlama model instead (no authentication needed)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Authentication & Model Loading\n",
                "This section handles:\n",
                "1. **HuggingFace Login**: Sets up credentials if using gated models (Meta Llama requires acceptance)\n",
                "2. **Model Selection**: Uses TinyLlama (open-source, no authentication needed)\n",
                "3. **Device Management**: Automatically selects CPU or GPU based on availability\n",
                "4. **Quantization**: Skipped on CPU to avoid complexity\n",
                "\n",
                "The model is loaded with `device_map=\"cpu\"` for compatibility with 4GB GPU constraints.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "GPU available, but using CPU for training due to memory constraints\n",
                        "(GPU training with 4GB VRAM is not feasible)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "`torch_dtype` is deprecated! Use `dtype` instead!\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model loaded on device: cpu\n"
                    ]
                }
            ],
            "source": [
                "# Model ID - We use 1B or 3B for local efficiency.\n",
                "# Option 1: Meta Llama (requires HF authentication - see previous cell)\n",
                "# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
                "\n",
                "# Option 2: Non-gated alternative (no authentication required)\n",
                "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
                "\n",
                "NEW_MODEL_NAME = \"llama-sentiment-classifier\"\n",
                "\n",
                "# For 4GB GPU, we'll use CPU training (slower but works without GPU memory issues)\n",
                "import torch\n",
                "USE_GPU = torch.cuda.is_available()\n",
                "\n",
                "if USE_GPU:\n",
                "    print(\"GPU available, but using CPU for training due to memory constraints\")\n",
                "    print(\"(GPU training with 4GB VRAM is not feasible)\")\n",
                "    device_map = \"cpu\"\n",
                "else:\n",
                "    device_map = \"cpu\"\n",
                "\n",
                "# QLoRA Configuration (4-bit quantization) - skip if using CPU\n",
                "if device_map == \"gpu\":\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.float16,\n",
                "        bnb_4bit_use_double_quant=False,\n",
                "    )\n",
                "    \n",
                "    # Load Base Model\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\"\n",
                "    )\n",
                "else:\n",
                "    # Load Base Model for CPU training (no quantization needed)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        device_map=\"cpu\",\n",
                "        torch_dtype=torch.float32,\n",
                "    )\n",
                "\n",
                "model.config.use_cache = False\n",
                "model.config.pretraining_tp = 1\n",
                "\n",
                "# Load Tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "print(f\"Model loaded on device: {device_map}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model Initialization Details\n",
                "**Model**: TinyLlama 1.1B parameters - compact and efficient for local training\n",
                "\n",
                "**Key configurations:**\n",
                "- `use_cache=False`: Disables KV cache to save memory\n",
                "- `pretraining_tp=1`: Single tensor parallel (for CPU training)\n",
                "- Tokenizer setup: Pad token set to EOS (End-Of-Sequence) token\n",
                "\n",
                "**Quantization skipped on CPU**: 4-bit quantization is primarily for GPU VRAM optimization.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Formatting the Dataset\n",
                "We convert the (Instruction, Input, Output) format into a single prompt string for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "06935309100d4cee811baa72490f1ca3",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Formatted dataset size: 100 samples\n",
                        "Sample formatted entry: ### Instruction:\n",
                        "Analyze the sentiment of the following movie review.\n",
                        "\n",
                        "### Input:\n",
                        "### Instruction:\n",
                        "Analyze the sentiment of the following movie review.\n",
                        "\n",
                        "### Input:\n",
                        "I rented I AM CURIOUS-YELLOW from my...\n"
                    ]
                }
            ],
            "source": [
                "def format_prompt(sample):\n",
                "    # Format for sentiment analysis\n",
                "    sentiment_label = \"positive\" if sample[\"label\"] == 1 else \"negative\"\n",
                "    text = f\"### Instruction:\\nAnalyze the sentiment of the following movie review.\\n\\n### Input:\\n{sample['text']}\\n\\n### Response:\\nSentiment: {sentiment_label}\"\n",
                "    return {\"text\": text}\n",
                "\n",
                "dataset = dataset.map(format_prompt)\n",
                "print(f\"Formatted dataset size: {len(dataset)} samples\")\n",
                "if len(dataset) > 0:\n",
                "    print(\"Sample formatted entry:\", dataset[0][\"text\"][:200] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Dataset Formatting\n",
                "We transform raw IMDB data into a standardized prompt format:\n",
                "\n",
                "**Format:**\n",
                "```\n",
                "### Instruction:\n",
                "Analyze the sentiment of the following movie review.\n",
                "\n",
                "### Input:\n",
                "[Full review text]\n",
                "\n",
                "### Response:\n",
                "Sentiment: [positive/negative]\n",
                "```\n",
                "\n",
                "This format teaches the model to:\n",
                "1. Understand the task (Instruction)\n",
                "2. Process the input (review text)\n",
                "3. Generate structured output (sentiment label)\n",
                "\n",
                "This is called \"Instruction Tuning\" - training on task-specific prompt-response pairs.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "peft_config = LoraConfig(\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0.1,\n",
                "    r=64,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### LoRA Configuration\n",
                "**LoRA (Low-Rank Adaptation)** is parameter-efficient fine-tuning. Instead of updating all 1.1B model weights, we only train small adapter matrices.\n",
                "\n",
                "**Key parameters:**\n",
                "- `lora_alpha=16`: Scaling factor for LoRA updates (higher = stronger updates)\n",
                "- `lora_dropout=0.1`: Regularization to prevent overfitting\n",
                "- `r=64`: Rank of the low-rank matrices (trade-off between expressivity and parameters)\n",
                "- `task_type=\"CAUSAL_LM\"`: We're doing causal language modeling (next-token prediction)\n",
                "\n",
                "**Benefits:**\n",
                "- ðŸš€ Requires only ~1% of original parameters to train\n",
                "- ðŸ’¾ Saves memory (fits in 4GB GPU)\n",
                "- âš¡ Faster training\n",
                "- ðŸ“¦ Produces tiny adapter files (~MB instead of GB)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training\n",
                "We use the `SFTTrainer` (Supervised Fine-tuning Trainer) from `trl`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training on CPU (this will be slower but will use less memory)\n",
                        "Total samples: 100\n",
                        "Estimated training time: ~5-10 minutes\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5e769bda54934e038bf60a83ac366506",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Adding EOS to train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6f28f8caac6347949ee5c5e91bcda21c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Tokenizing train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "034b3af981d444a5b6bf45106fa3048b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Truncating train dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
                        "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Starting training...\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [100/100 18:09, Epoch 1/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>2.688800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>2.705500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>2.365300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>2.265000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>2.348700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>12</td>\n",
                            "      <td>2.175500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>14</td>\n",
                            "      <td>2.257700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>16</td>\n",
                            "      <td>2.567400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>18</td>\n",
                            "      <td>2.021400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>20</td>\n",
                            "      <td>2.145900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>22</td>\n",
                            "      <td>2.623400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>24</td>\n",
                            "      <td>1.995800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>26</td>\n",
                            "      <td>2.057100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>28</td>\n",
                            "      <td>2.341100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>30</td>\n",
                            "      <td>2.286000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>32</td>\n",
                            "      <td>2.089400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>34</td>\n",
                            "      <td>1.757200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>36</td>\n",
                            "      <td>1.925600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>38</td>\n",
                            "      <td>2.313100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>40</td>\n",
                            "      <td>2.170000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>42</td>\n",
                            "      <td>2.193100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>44</td>\n",
                            "      <td>1.849500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>46</td>\n",
                            "      <td>2.017500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>48</td>\n",
                            "      <td>2.365900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>2.006400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>52</td>\n",
                            "      <td>2.149500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>54</td>\n",
                            "      <td>2.038700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>56</td>\n",
                            "      <td>1.711300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>58</td>\n",
                            "      <td>2.113100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>60</td>\n",
                            "      <td>1.899300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>62</td>\n",
                            "      <td>2.123900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>64</td>\n",
                            "      <td>2.107400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>66</td>\n",
                            "      <td>2.062000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>68</td>\n",
                            "      <td>2.064500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>70</td>\n",
                            "      <td>2.098000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>72</td>\n",
                            "      <td>1.734700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>74</td>\n",
                            "      <td>2.515000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>76</td>\n",
                            "      <td>1.923600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>78</td>\n",
                            "      <td>2.140700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>80</td>\n",
                            "      <td>2.165100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>82</td>\n",
                            "      <td>1.827700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>84</td>\n",
                            "      <td>2.345800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>86</td>\n",
                            "      <td>2.311900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>88</td>\n",
                            "      <td>2.166600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>90</td>\n",
                            "      <td>1.873600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>92</td>\n",
                            "      <td>2.208700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>94</td>\n",
                            "      <td>2.084200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>96</td>\n",
                            "      <td>1.823900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>98</td>\n",
                            "      <td>2.312000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>100</td>\n",
                            "      <td>2.000800</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/plain": [
                            "TrainOutput(global_step=100, training_loss=2.1466895055770876, metrics={'train_runtime': 1135.6586, 'train_samples_per_second': 0.088, 'train_steps_per_second': 0.088, 'total_flos': 257552055767040.0, 'train_loss': 2.1466895055770876})"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./results\",\n",
                "    num_train_epochs=1,\n",
                "    per_device_train_batch_size=1,  # Batch size of 1 for CPU\n",
                "    gradient_accumulation_steps=1,\n",
                "    optim=\"adamw_torch\",  # Use standard AdamW for CPU (paged_adamw_32bit only for GPU)\n",
                "    save_steps=10,\n",
                "    logging_steps=2,\n",
                "    learning_rate=2e-4,\n",
                "    weight_decay=0.001,\n",
                "    fp16=False,  # No mixed precision on CPU\n",
                "    bf16=False,\n",
                "    max_grad_norm=0.3,\n",
                "    max_steps=-1,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=False,  # Disable for CPU to avoid extra overhead\n",
                "    lr_scheduler_type=\"constant\",\n",
                "    report_to=\"none\",\n",
                "    no_cuda=True,  # Force CPU training\n",
                ")\n",
                "\n",
                "print(\"Training on CPU (this will be slower but will use less memory)\")\n",
                "print(f\"Total samples: {len(dataset)}\")\n",
                "print(f\"Estimated training time: ~5-10 minutes\")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    args=training_args,\n",
                ")\n",
                "\n",
                "# Start training\n",
                "print(\"\\nStarting training...\")\n",
                "trainer.train()\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Training Configuration & Execution\n",
                "**TrainingArguments** controls how the model is trained:\n",
                "\n",
                "**Key settings:**\n",
                "- `num_train_epochs=1`: One full pass through all 100 samples\n",
                "- `per_device_train_batch_size=1`: Process 1 sample at a time (memory constraint)\n",
                "- `learning_rate=2e-4`: How much to update weights per step\n",
                "- `fp16=False`: No mixed precision on CPU (keep everything as float32)\n",
                "- `no_cuda=True`: Force CPU training to avoid VRAM issues\n",
                "- `save_steps=10`: Save model every 10 training steps\n",
                "\n",
                "**Training time**: ~5-10 minutes on CPU for 100 samples\n",
                "\n",
                "**What happens during training:**\n",
                "1. Load batch of formatted data\n",
                "2. Forward pass: Model predicts next tokens\n",
                "3. Calculate loss: Compare predictions to true labels\n",
                "4. Backward pass: Compute gradients (how much to update)\n",
                "5. Update LoRA adapters (only 1% of parameters updated)\n",
                "6. Repeat for all 100 samples\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model adapter saved to locally at: llama-sentiment-classifier\n"
                    ]
                }
            ],
            "source": [
                "# Save the fine-tuned adapter\n",
                "trainer.model.save_pretrained(NEW_MODEL_NAME)\n",
                "tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
                "print(f\"Model adapter saved to locally at: {NEW_MODEL_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Saving the Fine-Tuned Adapter\n",
                "After training completes, we save:\n",
                "1. **LoRA adapter weights**: The trained low-rank matrices (small ~MB files)\n",
                "2. **Tokenizer**: The vocabulary and tokenization rules used\n",
                "\n",
                "**Why small files?** \n",
                "- Original model: 1.1B parameters Ã— 2 bytes (float16) = ~2.2 GB\n",
                "- LoRA adapters: Only rank-64 matrices for each layer = ~10-50 MB\n",
                "- Tokenizer: ~1 MB\n",
                "\n",
                "You can later combine these adapters with the original model for inference.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference Comparison (After Training)\n",
                "We now reload the model with the trained LoRA adapter to see the difference.\n",
                "\n",
                "*Note: To run this in Ollama (outside Python), you would typically fuse this adapter with the base model and convert it to GGUF format using `llama.cpp`.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "from peft import PeftModel\n",
                "\n",
                "# Load base model again (or reuse if memory allows, simpler to reload for clean state)\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    low_cpu_mem_usage=True,\n",
                "    return_dict=True,\n",
                "    torch_dtype=torch.float16,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "# Load the adapter we just trained\n",
                "ft_model = PeftModel.from_pretrained(base_model, NEW_MODEL_NAME)\n",
                "ft_model = ft_model.merge_and_unload() # Merge for faster inference\n",
                "\n",
                "ft_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
                "ft_tokenizer.pad_token = ft_tokenizer.eos_token"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Loading Fine-Tuned Model for Inference\n",
                "This cell prepares the trained model for testing:\n",
                "\n",
                "**Steps:**\n",
                "1. **Reload base model**: Load fresh TinyLlama from Hugging Face\n",
                "2. **Load adapters**: Inject the trained LoRA adapters we just saved\n",
                "3. **Merge and unload**: Fuse adapters into model weights for faster inference\n",
                "4. **Load tokenizer**: Use same tokenizer as training for consistency\n",
                "\n",
                "**Result**: A fully merged model that's ready to make predictions on new reviews.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Fine-Tuned Model Inference ---\n",
                        "Review Analysis: Analyze the sentiment of the following movie review: 'This film was absolutely amazing! The acting was brilliant, the storyline was engaging, and I couldn't take my eyes off the screen. I highly recommend it to everyone.'\n",
                        "\n",
                        "Response:\n",
                        "Sentiment: negative\n"
                    ]
                }
            ],
            "source": [
                "def query_finetuned(instruction, input_text=\"\"):\n",
                "    # Format prompt exactly as in training\n",
                "    if input_text:\n",
                "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input_text}\\n\\n### Response:\\n\"\n",
                "    else:\n",
                "        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n",
                "        \n",
                "    inputs = ft_tokenizer(prompt, return_tensors=\"pt\").to(base_model.device)\n",
                "    outputs = ft_model.generate(**inputs, max_new_tokens=200, use_cache=True)\n",
                "    response = ft_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    # Extract just the response part if possible\n",
                "    if \"### Response:\" in response:\n",
                "        return response.split(\"### Response:\")[1].strip()\n",
                "    return response\n",
                "\n",
                "print(\"--- Fine-Tuned Model Inference ---\")\n",
                "ft_response = query_finetuned(test_instruction)\n",
                "print(f\"Review Analysis: {test_instruction}\\n\")\n",
                "print(f\"Response:\\n{ft_response}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Running Inference with Fine-Tuned Model\n",
                "This function performs sentiment analysis on new movie reviews:\n",
                "\n",
                "**Process:**\n",
                "1. **Format prompt**: Build the same instruction-input-response template\n",
                "2. **Tokenize**: Convert text to token IDs using the tokenizer\n",
                "3. **Generate**: Model predicts next tokens to complete the response\n",
                "4. **Decode**: Convert token IDs back to human-readable text\n",
                "5. **Extract**: Pull out just the sentiment label from the response\n",
                "\n",
                "**Key parameters:**\n",
                "- `max_new_tokens=200`: Maximum tokens to generate (prevents infinite loops)\n",
                "- `to(base_model.device)`: Move data to same device as model (CPU or GPU)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Results Comparison\n",
                "Side-by-side view of the base Ollama model vs the Fine-Tuned Local model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "=== SENTIMENT ANALYSIS COMPARISON ===\n",
                        "\n",
                        "REVIEW: Analyze the sentiment of the following movie review: 'This film was absolutely amazing! The acting was brilliant, the storyline was engaging, and I couldn't take my eyes off the screen. I highly recommend it to everyone.'\n",
                        "\n",
                        "[BEFORE - Ollama Base Model]\n",
                        "The sentiment of this movie review is extremely positive. The reviewer uses superlatives such as \"absolutely amazing\" and \"brilliant\" to describe their experience with the film. They also explicitly state that they \"couldn't take my eyes off the screen,\" which suggests that the movie was so engaging and captivating that it held their full attention.\n",
                        "\n",
                        "The reviewer's use of the phrase \"I highly recommend it to everyone\" further emphasizes their enthusiasm for the film, implying that they think it is a must-see for anyone. There is no criticism or negative comment in the review, only praise and admiration for the movie.\n",
                        "\n",
                        "Overall, the sentiment of this review can be characterized as overwhelmingly positive and enthusiastic.\n",
                        "\n",
                        "------------------------------\n",
                        "\n",
                        "[AFTER - Fine-Tuned Adapter (IMDB)]\n",
                        "Sentiment: negative\n"
                    ]
                }
            ],
            "source": [
                "print(\"=== SENTIMENT ANALYSIS COMPARISON ===\\n\")\n",
                "print(f\"REVIEW: {test_instruction}\\n\")\n",
                "\n",
                "print(\"[BEFORE - Ollama Base Model]\")\n",
                "print(baseline_response)\n",
                "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
                "\n",
                "print(\"[AFTER - Fine-Tuned Adapter (IMDB)]\")\n",
                "print(ft_response)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Final Comparison: Before vs After Fine-Tuning\n",
                "This cell displays the key result:\n",
                "\n",
                "**What we're comparing:**\n",
                "1. **[BEFORE]**: Response from untrained Ollama model\n",
                "   - Has no knowledge of our specific sentiment task\n",
                "   - Provides generic analysis\n",
                "\n",
                "2. **[AFTER]**: Response from fine-tuned local model\n",
                "   - Trained on 100 IMDB movie reviews\n",
                "   - Should show more task-specific sentiment understanding\n",
                "   - Demonstrates the effect of fine-tuning\n",
                "\n",
                "**Interpretation:**\n",
                "- If models differ significantly, fine-tuning worked!\n",
                "- If similar, may indicate limited training data or need for hyperparameter tuning\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
